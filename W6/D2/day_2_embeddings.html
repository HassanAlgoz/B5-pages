<!DOCTYPE html>
<html lang="en"><head>
<link href="../../assets/icon.svg" rel="icon" type="image/svg+xml">
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.26">

  <meta name="author" content="malfadly@sdaia.gov.sa">
  <title>AI Pros Bootcamp – Day 2: Embeddings</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-2476b6dbe24137c74cb80772f2f63bf0.css">
  <link rel="stylesheet" href="../../assets/sdaia.scss">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <link rel="icon" href="../../assets/icon.svg">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="../../assets/anim.svg" data-background-opacity="0.1" class="quarto-title-block center">
  <h1 class="title">Day 2: Embeddings</h1>

<div class="quarto-title-authors">
</div>

</section>
<section id="module-overview" class="slide level2 smaller">
<h2>Module Overview</h2>
<p><strong>Session: Embeddings</strong></p>
<ul>
<li><a href="#/what-are-embeddings">What are embeddings and why they matter</a></li>
<li><a href="#/types-of-embeddings">Types of embeddings: text, image, structured, graph</a></li>
<li><a href="#/evaluating-embedding-quality">Evaluating embedding quality</a></li>
</ul>
<aside class="notes">
<p>Today we’re diving deep into embeddings - the foundation of modern retrieval systems. By the end of this session, you’ll understand how to transform any data type into numerical vectors.</p>
<p>This knowledge is critical because embeddings are what make semantic search possible. They’re what allow you to find relevant documents based on meaning, not just keyword matching.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section>
<section id="session-embeddings" class="title-slide slide level1 center">
<h1>Session: Embeddings</h1>

</section>
<section id="what-youll-learn-today" class="slide level2 smaller">
<h2>What You’ll Learn Today</h2>
<p><strong>By the end of this session, you will:</strong></p>
<ul>
<li>Understand embeddings as numerical representations of data</li>
<li>Know how to evaluate embedding quality</li>
<li>Recognize different types of embeddings and their use cases</li>
</ul>
<aside class="notes">
<p>Let me start by telling you what you’ll know at the end of this hour that you don’t know right now.</p>
<p>First, you’ll understand embeddings - not just as a buzzword, but as a concrete mathematical concept. You’ll see how text, images, and other data types can be transformed into vectors that capture semantic meaning.</p>
<p>Second, you’ll learn how to evaluate whether your embeddings are actually good. This is crucial because bad embeddings lead to bad search results.</p>
<p>Third, you’ll understand the different types of embeddings and when to use each one. Not all embeddings are created equal, and choosing the right one matters.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="what-are-embeddings" class="slide level2">
<h2>What Are Embeddings?</h2>
<p><strong>Numerical representations of real-world data</strong></p>

<img data-src="../assets/embeddings_concept.png" class="r-stretch quarto-figure-center"><p class="caption">Embeddings as vectors in a low-dimensional space representing text, images, and other data types</p><aside class="notes">
<p>Let’s start with the fundamental question: what are embeddings?</p>
<p>Embeddings are numerical representations of real-world data - text, images, audio, videos, anything. The name comes from mathematics, where one space can be mapped, or embedded, into another space.</p>
<p>For example, BERT embeds text into a vector of 768 numbers. This maps from the very high-dimensional space of all possible sentences to a much smaller 768-dimensional space.</p>
<p>The key insight is that embeddings are low-dimensional vectors where the geometric distance between two vectors reflects the semantic similarity between the real-world objects they represent. If two vectors are close in the embedding space, the objects they represent are semantically similar.</p>
<p>This is powerful because it gives you compact representations that retain important semantic properties while enabling efficient computation and storage.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="why-embeddings-matter" class="slide level2 smaller">
<h2>Why Embeddings Matter</h2>
<p><strong>Compact, meaningful representations</strong></p>
<ul>
<li><strong>Lossy compression</strong>: Reduce dimensionality while preserving semantics</li>
<li><strong>Similarity comparison</strong>: Measure relationships numerically</li>
<li><strong>Multimodal alignment</strong>: Map different data types to the same space</li>
<li><strong>Efficient computation</strong>: Enable fast search and retrieval</li>
</ul>
<aside class="notes">
<p>So why do embeddings matter? There are several key reasons.</p>
<p>First, they act as lossy compression. You’re reducing the dimensionality of your data - from potentially infinite-dimensional text space to a fixed-size vector - while preserving important semantic properties.</p>
<p>Second, they enable similarity comparison. You can measure how similar two objects are by computing the distance between their embeddings. This is much more powerful than exact matching.</p>
<p>Third, they enable multimodal alignment. You can map text, images, audio, and other data types into the same embedding space, allowing you to search across modalities - like finding videos based on text queries.</p>
<p>Fourth, they enable efficient computation. Once your data is in vector form, you can use optimized algorithms for search, clustering, and other operations that would be impossible or too slow on raw data.</p>
<p>These properties make embeddings essential for modern AI applications.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="embeddings-in-retrieval-systems" class="slide level2">
<h2>Embeddings in Retrieval Systems</h2>
<p><strong>The three-step process</strong></p>
<ol type="1">
<li><strong>Precompute</strong> embeddings for billions of items</li>
<li><strong>Map</strong> query embeddings into the same space</li>
<li><strong>Retrieve</strong> nearest neighbors efficiently</li>
</ol>
<aside class="notes">
<p>The power of embeddings becomes clear when you look at how modern retrieval systems work. Think about Google Search - it’s a retrieval task over the search space of the entire internet.</p>
<p>Today’s retrieval systems succeed through three steps:</p>
<p>First, you precompute embeddings for billions of items in your search space. This is done offline, so you can use expensive models and take your time.</p>
<p>Second, when a query comes in, you map it into the same embedding space. This is done in real-time, so you need fast models.</p>
<p>Third, you efficiently compute and retrieve items whose embeddings are nearest neighbors of the query embedding. This is where vector search algorithms and vector databases come in.</p>
<p>This three-step process is what makes semantic search possible at scale. Without embeddings, you’d be stuck with keyword matching. With embeddings, you can find semantically similar content even if it uses different words.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="joint-embeddings-for-multimodality" class="slide level2 smaller">
<h2>Joint Embeddings for Multimodality</h2>
<p><strong>Multiple data types in one space</strong></p>

<img data-src="../assets/joint_embedding_space.png" class="r-stretch quarto-figure-center"><p class="caption">Objects of different types (text, images, videos) projected into a joint vector space with semantic meaning</p><aside class="notes">
<p>One of the most powerful applications of embeddings is joint embeddings - mapping multiple types of objects into the same embedding space.</p>
<p>For example, you can map text, images, and videos into the same space. This means you can retrieve videos based on text queries, or find images that match a text description, or search for text that describes an image.</p>
<p>The key is that these embedding representations are designed to capture as much of the original object’s characteristics as possible, while placing objects with similar semantic properties closer in the embedding space.</p>
<p>This is what enables applications like image search with text queries, or finding relevant videos based on a description. The embeddings bridge the gap between different modalities.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="task-specific-embeddings" class="slide level2 smaller">
<h2>Task-Specific Embeddings</h2>

<img data-src="../assets/glove_vs_word2vec_embeddings.png" class="r-stretch quarto-figure-center"><p class="caption">2D visualization of pre-trained GloVe and Word2Vec word embeddings</p><p><strong>Different embeddings for different tasks</strong></p>
<ul>
<li>Same object → different embeddings</li>
<li>Optimized for the task at hand</li>
<li>Semantic meaning preserved for specific use cases</li>
</ul>
<aside class="notes">
<p>Here’s an important point: embeddings are task-specific. You can generate different embeddings for the same object, each optimized for a different task.</p>
<p>For example, you might have one embedding optimized for sentiment analysis, another for topic classification, and another for semantic search. The same text document would have different embeddings in each case.</p>
<p>This is both a feature and a challenge. It means you can optimize for your specific use case, but it also means you need to choose the right embedding model for your task.</p>
<p>The semantic meaning is preserved, but it’s preserved in a way that’s useful for your specific task. This is why evaluation is so important - you need to make sure your embeddings are actually good for what you’re trying to do.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="evaluating-embedding-quality" class="title-slide slide level1 center">
<h1>Evaluating Embedding Quality</h1>

</section>
<section id="why-evaluation-matters" class="slide level2">
<h2>Why Evaluation Matters</h2>
<p><strong>Bad embeddings → bad results</strong></p>
<ul>
<li>Embeddings are not automatically good</li>
<li>Quality depends on model, data, and task</li>
<li>Evaluation tells you if embeddings work for your use case</li>
</ul>
<aside class="notes">
<p>Before we dive into types of embeddings, let’s talk about evaluation. This is crucial because embeddings are not automatically good - their quality depends on the model, the data, and the task.</p>
<p>You can’t just grab any embedding model and assume it will work for your use case. You need to evaluate whether your embeddings are actually capturing the semantic relationships you care about.</p>
<p>Evaluation tells you if your embeddings work for your specific use case. It helps you choose the right model, tune parameters, and understand limitations.</p>
<p>Without evaluation, you’re flying blind. You might think your retrieval system is working, but you’re actually getting poor results because your embeddings don’t capture the right relationships.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="evaluation-methods" class="slide level2">
<h2>Evaluation Methods</h2>
<ul>
<li><strong>Intrinsic evaluation</strong>: Direct measurement of embedding properties</li>
<li><strong>Extrinsic evaluation</strong>: Performance on downstream tasks (IR, classification, ..etc.)</li>
<li><strong>Benchmarks</strong>: <a href="https://huggingface.co/spaces/mteb/leaderboard">MTEB</a>, BEIR for standardized comparison</li>
</ul>
<aside class="notes">
<p>There are several ways to evaluate embeddings:</p>
<p>Intrinsic evaluation measures the embedding properties directly - things like whether similar items are close in the embedding space, or whether the embeddings capture certain linguistic properties.</p>
<p>Extrinsic evaluation measures performance on downstream tasks - like how well your embeddings work for information retrieval, or classification, or clustering.</p>
<p>Benchmarks like MTEB (Massive Text Embedding Benchmark) and BEIR provide standardized ways to compare different embedding models. These are important because they give you a common ground for comparison.</p>
<p>The best approach is usually a combination - use benchmarks to narrow down your options, then do task-specific evaluation to find what works best for your use case.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="search-example-the-challenge" class="slide level2">
<h2>Search Example: The Challenge</h2>
<p><strong>Finding relevant documents</strong></p>
<ul>
<li>Query: “How do I reset my password?”</li>
<li>Corpus: Millions of support articles</li>
<li>Goal: Find the most relevant articles</li>
</ul>
<aside class="notes">
<p>Let me give you a concrete example of why embedding quality matters. Imagine you’re building a search system for a support knowledge base.</p>
<p>A user asks: “How do I reset my password?”</p>
<p>You have millions of support articles in your corpus. Your goal is to find the most relevant articles.</p>
<p>With keyword matching, you might find articles that mention “reset” and “password” but miss articles that say “change your login credentials” or “forgot password recovery.”</p>
<p>With good embeddings, you should find all semantically similar articles, even if they use different words. But with bad embeddings, you might miss relevant articles or get irrelevant ones.</p>
<p>This is why evaluation is so important - you need to know if your embeddings are actually finding the right documents.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="types-of-embeddings" class="title-slide slide level1 center">
<h1>Types of Embeddings</h1>

</section>
<section id="text-embeddings-overview" class="slide level2">
<h2>Text Embeddings Overview</h2>
<p><strong>From words to documents</strong></p>
<ul>
<li><strong>Word embeddings</strong>: Individual words → vectors</li>
<li><strong>Document embeddings</strong>: Entire documents → vectors</li>
<li><strong>Context-aware</strong>: Meaning depends on surrounding text</li>
</ul>
<aside class="notes">
<p>Now let’s talk about types of embeddings. We’ll start with text embeddings, which are probably the most common.</p>
<p>Text embeddings come in different flavors. Word embeddings map individual words to vectors. Document embeddings map entire documents to vectors. And modern embeddings are context-aware - the meaning depends on the surrounding text.</p>
<p>The evolution of text embeddings tells an important story about how the field has progressed from simple bag-of-words models to sophisticated neural networks that understand context.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="word-embeddings" class="slide level2">
<h2>Word Embeddings</h2>
<p><strong>Individual words as vectors</strong></p>
<ul>
<li><strong>Word2Vec</strong>: Predicts words from context</li>
<li><strong>GloVe</strong>: Global word-word co-occurrence statistics</li>
<li><strong>FastText</strong>: Handles out-of-vocabulary words</li>
</ul>

<img data-src="../assets/word_embeddings.png" class="r-stretch quarto-figure-center"><p class="caption">Word embeddings showing similar words clustered together in vector space</p><aside class="notes">
<p>Word embeddings were one of the first breakthroughs in NLP. They map individual words to fixed-size vectors.</p>
<p>Word2Vec was one of the earliest and most influential. It learns embeddings by predicting words from their context - words that appear in similar contexts get similar embeddings.</p>
<p>GloVe takes a different approach - it uses global word-word co-occurrence statistics from a corpus. It’s based on the idea that words that co-occur frequently are likely related.</p>
<p>FastText extends Word2Vec by representing words as bags of character n-grams. This means it can handle out-of-vocabulary words by breaking them into subword units.</p>
<p>The key insight is that these embeddings capture semantic relationships - words with similar meanings end up close in the embedding space. You can do arithmetic on them - “king” minus “man” plus “woman” gives you something close to “queen.”</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="document-embeddings-shallow-models" class="slide level2">
<h2>Document Embeddings: Shallow Models</h2>
<p><strong>Bag-of-words approaches</strong></p>
<ul>
<li><strong>TF-IDF</strong>: Term frequency × inverse document frequency</li>
<li><strong>BM25</strong>: Probabilistic ranking function</li>
<li><strong>Limitations</strong>: No word order, no context</li>
</ul>
<aside class="notes">
<p>Before deep learning, document embeddings were mostly based on bag-of-words approaches. These are called “shallow” because they don’t use deep neural networks.</p>
<p>TF-IDF - Term Frequency times Inverse Document Frequency - is a classic approach. It weights words by how common they are in the document relative to how common they are across all documents. Common words get lower weights.</p>
<p>BM25 is a probabilistic ranking function that’s still widely used today, especially in search engines. It’s an improvement over TF-IDF that handles document length normalization better.</p>
<p>The main limitation of these approaches is that they ignore word order and context. “The cat sat on the mat” and “The mat sat on the cat” would have the same representation, even though they mean completely different things.</p>
<p>They also can’t capture semantic relationships - “car” and “automobile” are treated as completely different words.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="document-embeddings-deep-models" class="slide level2">
<h2>Document Embeddings: Deep Models</h2>
<p><strong>Context-aware neural embeddings</strong></p>
<ul>
<li><strong>BERT</strong>: Bidirectional encoder representations</li>
<li><strong>Sentence-BERT</strong>: Efficient sentence embeddings</li>
<li><strong>Modern LLMs</strong>: GPT, T5, Gemini embeddings</li>
</ul>

<img data-src="../assets/single-vector_vs_multi-vector_encoders.png" class="r-stretch quarto-figure-center"><p class="caption">Single-vector vs.&nbsp;Multi-vector encoders</p><aside class="notes">
<p>Deep models changed everything. They use neural networks to learn embeddings that capture context and semantic meaning.</p>
<p>BERT - Bidirectional Encoder Representations from Transformers - was a breakthrough. It reads text in both directions simultaneously, so it understands context. The word “bank” gets different embeddings depending on whether it’s “river bank” or “financial bank.”</p>
<p>Sentence-BERT makes BERT practical for sentence-level tasks. Regular BERT requires running the full model for every sentence pair you want to compare, which is too slow. Sentence-BERT fine-tunes BERT to produce sentence embeddings directly, making similarity comparison much faster.</p>
<p>Modern LLMs like GPT, T5, and Gemini can also produce embeddings. These are often very high quality because they’re trained on massive amounts of data, but they can be expensive to run.</p>
<p>The key advantage of deep models is that they understand context and semantics. They can tell that “car” and “automobile” are similar, and that “The cat sat on the mat” is different from “The mat sat on the cat.”</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="image-multimodal-embeddings" class="slide level2 smaller">
<h2>Image &amp; Multimodal Embeddings</h2>
<p><strong>Visual understanding in vector space</strong></p>
<ul>
<li><strong>CNN-based</strong>: Convolutional neural networks for images</li>
<li><strong>Vision transformers</strong>: ViT, <a href="https://github.com/openai/CLIP">CLIP</a> for joint text-image space</li>
<li><strong>Multimodal</strong>: Same space for text, images, videos</li>
</ul>

<img data-src="../assets/open_ai_clip_model.png" class="r-stretch quarto-figure-center"><p class="caption">Images and text projected into a joint embedding matrix (OpenAI’s CLIP model)</p><aside class="notes">
<p>Images can also be embedded into vectors. Early approaches used CNNs - convolutional neural networks - trained on image classification tasks. The final layers of these networks produce image embeddings.</p>
<p>Vision transformers, like ViT, apply the transformer architecture to images by splitting them into patches. These often work better than CNNs for many tasks.</p>
<p>CLIP is particularly interesting - it learns a joint embedding space for images and text. You can embed both an image and a text description into the same space, and similar images and texts will be close together. This enables powerful applications like image search with text queries.</p>
<p>Multimodal embeddings extend this to videos, audio, and other data types. The goal is always the same: map different modalities into the same space so you can search and compare across them.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="structured-data-embeddings" class="slide level2">
<h2>Structured Data Embeddings</h2>
<p><strong>Tables, graphs, and relational data</strong></p>
<ul>
<li><strong>General structured data</strong>: Feature engineering + ML</li>
<li><strong>User-item data</strong>: Collaborative filtering embeddings</li>
<li><strong>Graph embeddings</strong>: Node and edge representations</li>
</ul>
<aside class="notes">
<p>Structured data - like tables, databases, and graphs - can also be embedded.</p>
<p>For general structured data, you typically do feature engineering to convert rows into feature vectors, then use machine learning models to learn embeddings. This is common in recommendation systems.</p>
<p>For user-item data, collaborative filtering learns embeddings for users and items simultaneously. Users with similar preferences get similar embeddings, and items that are liked by similar users get similar embeddings.</p>
<p>Graph embeddings are particularly interesting. They embed nodes and edges in a graph into vectors, preserving the graph structure. Nodes that are connected or have similar neighborhoods get similar embeddings. This is useful for social networks, knowledge graphs, and other graph-structured data.</p>
<p>The key insight is that embeddings aren’t just for text and images - you can embed any structured data into vectors if you design the right approach.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="training-embeddings" class="title-slide slide level1 center">
<h1>Training Embeddings</h1>

</section>
<section id="training-approaches" class="slide level2">
<h2>Training Approaches</h2>
<p><strong>How embeddings are learned</strong></p>
<ul>
<li><strong>Supervised</strong>: Task-specific labels guide learning</li>
<li><strong>Self-supervised</strong>: Learn from data structure itself</li>
<li><strong>Transfer learning</strong>: Pre-trained models fine-tuned</li>
</ul>
<aside class="notes">
<p>Now let’s talk about how embeddings are actually trained. There are several approaches:</p>
<p>Supervised training uses task-specific labels. For example, you might train embeddings for sentiment analysis by using labeled examples of positive and negative text. The embeddings learn to separate positive and negative examples.</p>
<p>Self-supervised training learns from the data structure itself, without explicit labels. Word2Vec is self-supervised - it learns from word co-occurrence patterns. BERT is also self-supervised - it learns by predicting masked words.</p>
<p>Transfer learning takes a pre-trained model and fine-tunes it for your specific task. This is often the most practical approach - you start with a model trained on massive amounts of data, then fine-tune it for your use case.</p>
<p>The choice depends on your data, your task, and your resources. Pre-trained models are often the best starting point because they’ve learned from much more data than you have.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="pre-trained-vs.-custom-embeddings" class="slide level2">
<h2>Pre-trained vs.&nbsp;Custom Embeddings</h2>
<p><strong>When to train your own</strong></p>
<ul>
<li><strong>Pre-trained</strong>: Use when available, often better</li>
<li><strong>Custom</strong>: Needed for domain-specific tasks</li>
<li><strong>Fine-tuning</strong>: Best of both worlds</li>
</ul>
<aside class="notes">
<p>Should you use pre-trained embeddings or train your own?</p>
<p>Pre-trained embeddings are usually the better choice. They’re trained on massive datasets, they’re well-tested, and they often perform better than what you could train yourself. Unless you have a very specific domain or very specific requirements, start with pre-trained.</p>
<p>Custom embeddings make sense when you have domain-specific data that’s very different from what pre-trained models were trained on. For example, if you’re working with medical text, legal documents, or highly technical content, domain-specific embeddings might help.</p>
<p>Fine-tuning is often the best approach - start with a pre-trained model, then fine-tune it on your domain-specific data. This gives you the benefits of both: the general knowledge from pre-training and the domain specificity from fine-tuning.</p>
<p>The key is to evaluate. Don’t assume custom is better - test both and see what works for your use case.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../../assets/icon.svg" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"},{"title":"Index","icon":"<svg width='1.8rem' viewBox='0 0 24 24'><path d='M5 7h14M5 12h14M5 17h7' stroke='white' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'/></svg>","src":"../index.html"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: true,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>